{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4809702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# display\n",
    "from IPython.display import display\n",
    "\n",
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# fix random seed\n",
    "from numpy.random import seed as set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "# explainability\n",
    "# import shap, lime, eli5\n",
    "# shap.initjs()\n",
    "\n",
    "#mlflow\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "\n",
    "import streamlit as st\n",
    "def cache(f):\n",
    "    return f\n",
    "st.cache = cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62ffba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_chat\n",
    "chat = read_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9773e04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-10 13:45:00</td>\n",
       "      <td>Alon Wolf </td>\n",
       "      <td> 转专 拽砖 砖 专砖\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-10 13:45:00</td>\n",
       "      <td>Alon Wolf </td>\n",
       "      <td>驻 拽转 - 转- - '砖 砖拽砖专 '\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-10 13:46:00</td>\n",
       "      <td>Alon Wolf </td>\n",
       "      <td> \\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-10 13:47:00</td>\n",
       "      <td>专 状爪拽转状 </td>\n",
       "      <td>  砖 专转!\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-10 13:47:00</td>\n",
       "      <td>专 状爪拽转状 </td>\n",
       "      <td>转注砖 专砖 转 驻驻\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>2020-11-20 07:46:00</td>\n",
       "      <td>Alon Wolf </td>\n",
       "      <td> *  *  砖专爪转 砖 \\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>2020-11-20 07:46:00</td>\n",
       "      <td>Alon Wolf </td>\n",
       "      <td>&lt;  &gt;\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>2020-11-20 07:47:00</td>\n",
       "      <td>Alon Wolf </td>\n",
       "      <td> 祝 (:\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>2020-11-20 07:47:00</td>\n",
       "      <td>Alon Wolf </td>\n",
       "      <td>转专  砖 转  \\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>2020-11-20 08:12:00</td>\n",
       "      <td>Alon Wolf </td>\n",
       "      <td>&lt;  &gt;\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39999 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime                 name  \\\n",
       "0     2018-05-10 13:45:00          Alon Wolf    \n",
       "1     2018-05-10 13:45:00          Alon Wolf    \n",
       "2     2018-05-10 13:46:00          Alon Wolf    \n",
       "3     2018-05-10 13:47:00  专 状爪拽转状    \n",
       "4     2018-05-10 13:47:00  专 状爪拽转状    \n",
       "...                   ...                  ...   \n",
       "39994 2020-11-20 07:46:00          Alon Wolf    \n",
       "39995 2020-11-20 07:46:00          Alon Wolf    \n",
       "39996 2020-11-20 07:47:00          Alon Wolf    \n",
       "39997 2020-11-20 07:47:00          Alon Wolf    \n",
       "39998 2020-11-20 08:12:00          Alon Wolf    \n",
       "\n",
       "                                                    text  \n",
       "0                         转专 拽砖 砖 专砖\\r  \n",
       "1      驻 拽转 - 转- - '砖 砖拽砖专 '\\r\\n...  \n",
       "2                                             \\r  \n",
       "3                                      砖 专转!\\r  \n",
       "4                              转注砖 专砖 转 驻驻\\r  \n",
       "...                                                  ...  \n",
       "39994                    *  *  砖专爪转 砖 \\r  \n",
       "39995                                 <  >\\r  \n",
       "39996                                       祝 (:\\r  \n",
       "39997                     转专  砖 转  \\r  \n",
       "39998                                 <  >\\r  \n",
       "\n",
       "[39999 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "585f9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac1aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MEDIA_TOKEN = 'MEDIA'\n",
    "URL_TOKEN = 'URL'\n",
    "TOKENS = [MEDIA_TOKEN, URL_TOKEN]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('<  >', MEDIA_TOKEN, text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', URL_TOKEN, text)\n",
    "    return text[:-1] # removes end\n",
    "text = chat['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f12b81e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             转专 拽砖 砖 专砖\n",
       "1        驻 拽转 - 转- - '砖 砖拽砖专 '\\r\\n...\n",
       "2                                                 \n",
       "3                                          砖 专转!\n",
       "4                                  转注砖 专砖 转 驻驻\n",
       "                               ...                        \n",
       "39994                        *  *  砖专爪转 砖 \n",
       "39995                                                MEDIA\n",
       "39996                                           祝 (:\n",
       "39997                         转专  砖 转  \n",
       "39998                                                MEDIA\n",
       "Name: text, Length: 39999, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2d820c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extras.nlp.hebtokenizer import tokenize as nlp_tokenize\n",
    "\n",
    "def tokenize_text(txt):\n",
    "    tok = [word for lng, word in nlp_tokenize(txt)]\n",
    "    return ['###'] + tok + ['$$$']\n",
    "\n",
    "tokenized = text.apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "014eb77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = tokenized.apply(len).max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9866266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [###, , 转专, 拽砖, 砖, 专砖, $$$]\n",
       "1        [###, 驻, 拽转, -, 转-, -, ', 砖, 砖拽...\n",
       "2                                   [###, , , $$$]\n",
       "3                       [###, , , 砖, 专转, !, $$$]\n",
       "4                   [###, 转注砖, 专砖, 转, 驻驻, $$$]\n",
       "                               ...                        \n",
       "39994    [###, , * , , * , , 砖专爪转, 砖, ,...\n",
       "39995                                    [###, MEDIA, $$$]\n",
       "39996                          [###, , 祝, (, :, $$$]\n",
       "39997        [###, 转专, , 砖, 转, , , $$$]\n",
       "39998                                    [###, MEDIA, $$$]\n",
       "Name: text, Length: 39999, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc69e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with mlflow.start_run(run_name=\"test\"+ str(datetime.now())):\n",
    "#      mlflow.log_param('n_features', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ff9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment(experiment_name, **params):\n",
    "    with mlflow.start_run(run_name= experiment_name+ '_' +str(datetime.now())):\n",
    "        for k, v in params.items():\n",
    "            mlflow.log_param(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7e763a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d045189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd5c39ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2071214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83f863b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = text\n",
    "TOKENIZED = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ada85a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args = {},\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def load_words(self):\n",
    "        return TOKENIZED.apply(lambda l : ' '.join(l))\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.args['sequence_length']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.args['sequence_length']]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.args['sequence_length']+1]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32104d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "# from model import Model\n",
    "# from dataset import Dataset\n",
    "\n",
    "def train(dataset, model, args):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=args['batch_size'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(args['max_epochs']):\n",
    "        state_h, state_c = model.init_state(args['sequence_length'])\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "            \n",
    "def predict(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'batch': 0, 'loss': 10.138138771057129}\n",
      "{'epoch': 0, 'batch': 1, 'loss': 10.123454093933105}\n",
      "{'epoch': 0, 'batch': 2, 'loss': 10.108195304870605}\n",
      "{'epoch': 0, 'batch': 3, 'loss': 10.091282844543457}\n",
      "{'epoch': 0, 'batch': 4, 'loss': 10.07086181640625}\n",
      "{'epoch': 0, 'batch': 5, 'loss': 10.043609619140625}\n",
      "{'epoch': 0, 'batch': 6, 'loss': 10.003386497497559}\n",
      "{'epoch': 0, 'batch': 7, 'loss': 9.936370849609375}\n",
      "{'epoch': 0, 'batch': 8, 'loss': 9.820688247680664}\n",
      "{'epoch': 0, 'batch': 9, 'loss': 9.643900871276855}\n",
      "{'epoch': 0, 'batch': 10, 'loss': 9.406193733215332}\n",
      "{'epoch': 0, 'batch': 11, 'loss': 9.14319133758545}\n",
      "{'epoch': 0, 'batch': 12, 'loss': 8.913790702819824}\n",
      "{'epoch': 0, 'batch': 13, 'loss': 8.714279174804688}\n",
      "{'epoch': 0, 'batch': 14, 'loss': 8.538949012756348}\n",
      "{'epoch': 0, 'batch': 15, 'loss': 8.399476051330566}\n",
      "{'epoch': 0, 'batch': 16, 'loss': 8.312835693359375}\n",
      "{'epoch': 0, 'batch': 17, 'loss': 8.234450340270996}\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--max-epochs', type=int, default=10)\n",
    "# parser.add_argument('--batch-size', type=int, default=256)\n",
    "# parser.add_argument('--sequence-length', type=int, default=4)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = {\n",
    "    'max_epochs' : 10,\n",
    "    'batch_size' : 32,\n",
    "    'sequence_length' : max_len,\n",
    "    \n",
    "}\n",
    "\n",
    "dataset = Dataset(args)\n",
    "model = Model(dataset)\n",
    "\n",
    "train(dataset, model, args)\n",
    "print(predict(dataset, model, text='拽专 '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f4c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
